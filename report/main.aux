\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gene}
\citation{whatisGene}
\citation{globalViewOfGene}
\citation{genegene}
\citation{geneLimit}
\citation{MLinGene}
\citation{geneProbe}
\citation{pca}
\citation{knn}
\citation{svm}
\citation{deepLearning1}
\citation{BP}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{\hskip -1em.~Introduction}{section.1}{}}
\citation{crossValidation}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Approaches}{2}{section.2}}
\newlabel{sec:approaches}{{2}{2}{\hskip -1em.~Approaches}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Dimension Reduction}{2}{subsection.2.1}}
\newlabel{eq:mean}{{1}{2}{\hskip -1em.~Dimension Reduction}{equation.2.1}{}}
\newlabel{eq:cx}{{2}{2}{\hskip -1em.~Dimension Reduction}{equation.2.2}{}}
\newlabel{eq:eig}{{3}{2}{\hskip -1em.~Dimension Reduction}{equation.2.3}{}}
\newlabel{eq:dimR}{{4}{2}{\hskip -1em.~Dimension Reduction}{equation.2.4}{}}
\newlabel{eq:proj}{{5}{2}{\hskip -1em.~Dimension Reduction}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Classical Methods}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}K-nearest Neighbors Algorithm}{2}{subsubsection.2.2.1}}
\newlabel{eq:l1}{{6}{2}{K-nearest Neighbors Algorithm}{equation.2.6}{}}
\citation{svmCS231n}
\citation{svmCS231n}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces K-nearest neighbors algorithm}}{3}{algocf.1}}
\newlabel{alg:knn}{{1}{3}{K-nearest Neighbors Algorithm}{algocf.1}{}}
\newlabel{eq:l2}{{7}{3}{K-nearest Neighbors Algorithm}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Linear Classification}{3}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Score Function}{3}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{3}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Hinge Loss (SVM)}{3}{section*.3}}
\newlabel{eq:svmloss}{{9}{3}{Hinge Loss (SVM)}{equation.2.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-entropy Loss (Softmax)}{3}{section*.4}}
\citation{KLdivergence}
\citation{svmCS231n}
\citation{neuronModel}
\@writefile{toc}{\contentsline {paragraph}{Regularization}{4}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Deep Learning}{4}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {paragraph}{The Neuron Model}{4}{section*.6}}
\citation{neuronModel}
\citation{relu}
\citation{sigmoid}
\citation{weightInit}
\citation{svmCS231n}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Biological motivation of the neuron model. \cite  {neuronModel}}}{5}{figure.1}}
\newlabel{fig:neurons}{{1}{5}{Biological motivation of the neuron model. \cite {neuronModel}}{figure.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A biological neuron}}}{5}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A common mathematical model}}}{5}{figure.1}}
\@writefile{toc}{\contentsline {paragraph}{Activation Function}{5}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Neural Network Architectures}{5}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Weight Initialization}{5}{section*.9}}
\citation{Zou2005Zou}
\@writefile{toc}{\contentsline {paragraph}{Regularization}{6}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Loss Functions}{6}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Experiments and Results}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Preprocess}{6}{subsection.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Size of the trainset and test set}}{6}{table.1}}
\newlabel{tab:crossV}{{1}{6}{Size of the trainset and test set}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Principal Components Analysis}{6}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The accumulated percentage-dimension curve.}}{6}{figure.2}}
\newlabel{fig:pca}{{2}{6}{The accumulated percentage-dimension curve}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Dimensions required by different variance percentages}}{6}{table.2}}
\newlabel{tab:pca}{{2}{6}{Dimensions required by different variance percentages}{table.2}{}}
\citation{adam}
\citation{dropout}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}K-Nearest Neighbors}{7}{subsection.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performances of k-NN with different parameters}}{7}{table.3}}
\newlabel{tab:knn}{{3}{7}{Performances of k-NN with different parameters}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Linear classifier}{7}{subsection.3.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Performances of linear classifier with different loss functions}}{7}{table.4}}
\newlabel{tab:lc}{{4}{7}{Performances of linear classifier with different loss functions}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.\nobreakspace  {}Deep learning}{7}{subsection.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning curves of linear classifier with SVM loss.}}{7}{figure.3}}
\newlabel{fig:lc_svm}{{3}{7}{Learning curves of linear classifier with SVM loss}{figure.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training Accuracy}}}{7}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test Accuracy}}}{7}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Loss}}}{7}{figure.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning curves of linear classifier with Softmax loss.}}{7}{figure.4}}
\newlabel{fig:lc_softmax}{{4}{7}{Learning curves of linear classifier with Softmax loss}{figure.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training Accuracy}}}{7}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test Accuracy}}}{7}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Loss}}}{7}{figure.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Performances of neural network with different loss functions}}{7}{table.5}}
\newlabel{tab:dl}{{5}{7}{Performances of neural network with different loss functions}{table.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Network Architecture.}}{8}{figure.5}}
\newlabel{fig:net_architecture}{{5}{8}{Network Architecture}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning curves of neural network with SVM loss.}}{8}{figure.6}}
\newlabel{fig:dl_svm}{{6}{8}{Learning curves of neural network with SVM loss}{figure.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training Accuracy}}}{8}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test Accuracy}}}{8}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Loss}}}{8}{figure.6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Discussion}{8}{section.4}}
\@writefile{toc}{\contentsline {paragraph}{PCA Parameter Choosing}{8}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{K-NN Parameter Choosing}{8}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning curves of neural network with Softmax loss.}}{8}{figure.7}}
\newlabel{fig:dl_softmax}{{7}{8}{Learning curves of neural network with Softmax loss}{figure.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Training Accuracy}}}{8}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Test Accuracy}}}{8}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Loss}}}{8}{figure.7}}
\@writefile{toc}{\contentsline {paragraph}{Deep Learning Parameter Choosing}{8}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Loss Functions Selection}{8}{section*.15}}
\citation{transferLearning}
\citation{*}
\bibstyle{ieee}
\bibdata{reference}
\bibcite{geneLimit}{1}
\bibcite{genegene}{2}
\bibcite{sigmoid}{3}
\bibcite{gene}{4}
\bibcite{knn}{5}
\bibcite{svmCS231n}{6}
\bibcite{neuronModel}{7}
\bibcite{weightInit}{8}
\bibcite{adam}{9}
\bibcite{geneProbe}{10}
\bibcite{MLinGene}{11}
\bibcite{KLdivergence}{12}
\bibcite{BP}{13}
\bibcite{deepLearning}{14}
\bibcite{deepLearning1}{15}
\bibcite{globalViewOfGene}{16}
\bibcite{crossValidation}{17}
\bibcite{relu}{18}
\bibcite{transferLearning}{19}
\bibcite{pytorch}{20}
\@writefile{toc}{\contentsline {paragraph}{Linear Classifer \& Kernel-based Classifer}{9}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Classical Methods \& Deep Learning}{9}{section*.17}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion and Thinking}{9}{section.5}}
\bibcite{whatisGene}{21}
\bibcite{dropout}{22}
\bibcite{svm}{23}
\bibcite{pca}{24}
\bibcite{Zou2005Zou}{25}
